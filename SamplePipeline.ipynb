{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "center-education",
   "metadata": {},
   "source": [
    "# Azure ML Pipeline\n",
    "This notebook demonstrates creation & execution of an Azure ML pipeline designed to query data from an attached Azure SQL Datastore (using dynamic arguments passed as a pipeline parameter), process that data, and then export the result dataset to ADLS Gen 2 and the attached Azure SQL DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-sunday",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-catering",
   "metadata": {},
   "source": [
    "### Connect to Azure ML Workspace, Provision Compute Resources, and get References to Datastores\n",
    "\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing. To enable data insertion into Azure SQL DB we use a DataTransferStep in our pipeline - this step requires a DataFactoryCompute target which is provisioned below. Finally, get references to your ADLS Gen2, Azure SQL, and default blob datastores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=10)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Create Data Factory Compute for DataTransferStep\n",
    "def get_or_create_data_factory(workspace, factory_name):\n",
    "    try:\n",
    "        return DataFactoryCompute(workspace, factory_name)\n",
    "    except ComputeTargetException as e:\n",
    "        if 'ComputeTargetNotFound' in e.message:\n",
    "            print('Data factory not found, creating...')\n",
    "            provisioning_config = DataFactoryCompute.provisioning_configuration()\n",
    "            data_factory = ComputeTarget.create(workspace, factory_name, provisioning_config)\n",
    "            data_factory.wait_for_completion()\n",
    "            return data_factory\n",
    "        else:\n",
    "            raise e\n",
    "data_factory_name = 'adfcompute'           \n",
    "data_factory_compute = get_or_create_data_factory(ws, data_factory_name)\n",
    "\n",
    "#Get Default, Azure SQL, and ADLS Gen2 Datastores\n",
    "default_ds = ws.get_default_datastore()\n",
    "azsql_ds = Datastore.get(ws, 'azsql_ds')\n",
    "adlsgen2_ds = Datastore.get(ws, 'azadlsgen2_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-salad",
   "metadata": {},
   "source": [
    "### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py).\n",
    "~~~~\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['requests'])\n",
    "run_config.environment.python.conda_dependencies.add_pip_package('azureml-opendatasets')\n",
    "~~~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-reward",
   "metadata": {},
   "source": [
    "### Define Output Datasets\n",
    "Below we define the configuration for datasets that will be passed between steps in our pipeline. Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the `register_on_complete()` call. We also leverage a `PipelineData` object as an intermediary before saving that data to ADLS Gen2 and Azure SQL DB. PipelineData objects are compatible inputs to a `TransferDataStep` and make exporting data to Azure SQL DB easier - the `SqlDataReference` references the target Azure SQL DB and table we aim to insert data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_dataset = OutputFileDatasetConfig(name='profile_data', destination=(default_ds, 'profile_data/{run-id}')).read_delimited_files().register_on_complete(name='profile_data')\n",
    "filter_dataset = OutputFileDatasetConfig(name='filter_data', destination=(default_ds, 'filter_data/{run-id}')).read_delimited_files().register_on_complete(name='filter_data')\n",
    "processed_dataset_interim = PipelineData('processed_data_interim', datastore=default_ds)\n",
    "processed_dataset = OutputFileDatasetConfig(name='processed_data', destination=(adlsgen2_ds, '{run-id}')).read_delimited_files().register_on_complete(name='processed_data')\n",
    "sql_data_ref = SqlDataReference(datastore=azsql_ds, sql_table=\"Results\", data_reference_name='azure_sql_reference')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-grace",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "`PipelineParameter` objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we specify a single `filter_param` pipeline parameter which will be used as an argument in a SQL query, and to create a unique filename when writing data to ADLS Gen 2. Multiple pipeline parameters can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_param = PipelineParameter(name='filter_parameter', default_value=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-spice",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of five steps - two queries to Azure SQL DB, a processing step where ML code should go, an export to ADLS Gen 2, and a data transfer to Azure SQL DB. All of the `PythonScriptStep`s have a corresponding `*.py` file which is referenced in the step arguments. Also, any `PipelineParameter`s define above can be passed to and consumed within these steps. The final `DataTransferStep` is used to move data from blob storage to Azure SQL DB. <b>NOTE:</b> in order for the `DataTransferStep` to work your Azure SQL DB datastore must be connected via a service principal - [see the documentation here for more details](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.datatransferstep?view=azure-ml-py#remarks) and [here for details on configuring service principal authentication](https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database#service-principal-authentication). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL query to get 'filter' data. Note: filter_param is passed as an argument here and used to modify the SQL query.\n",
    "#Output data is collected in the filter_dataset, registered upon step completion, and passed to subsequent steps.\n",
    "get_filter_data_step = PythonScriptStep(\n",
    "    name='get-filter-data',\n",
    "    script_name='get_sql_filter_data.py',\n",
    "    arguments =['--query_param', filter_param,\n",
    "               '--filter_dataset', filter_dataset],\n",
    "    outputs=[filter_dataset],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='.',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "#SQL query to get 'profile' data.\n",
    "#Data is collected in the profile_dataset, registered upon step completion, and passed to subsequent steps.\n",
    "get_profile_data_step = PythonScriptStep(\n",
    "    name='get-profile-data',\n",
    "    script_name='get_sql_profile_data.py',\n",
    "    arguments =['--profile_dataset', profile_dataset],\n",
    "    outputs=[profile_dataset],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='.',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "#This step should contain all of the ML goodness.\n",
    "#filter_dataset & profile_dataset are passed in and parsed into pandas dataframes.\n",
    "#Output data is collected in the processed_dataset_interim PipelineData object\n",
    "#which is subsequently exported to ADLS Gen2 and Azure SQL DB\n",
    "process_data_step = PythonScriptStep(\n",
    "    name='process-data',\n",
    "    script_name='process_data.py',\n",
    "    arguments =['--processed_dataset_interim', processed_dataset_interim],\n",
    "    inputs=[filter_dataset.as_input('filter_data'), profile_dataset.as_input('profile_data')],\n",
    "    outputs=[processed_dataset_interim],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='.',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "#Write processed results to ADLS Gen2.\n",
    "#processed_dataset_interim is passed in and parsed into a pandas dataframe.\n",
    "#Output data is collected and registered in the processed_dataset which is written to ADLS Gen2.\n",
    "#Output data is registered upon step completion.\n",
    "save_data_adls_step = PythonScriptStep(\n",
    "    name='save-data-adls',\n",
    "    script_name='save_data_adls_gen2.py',\n",
    "    arguments =['--processed_dataset', processed_dataset, '--query_param', filter_param, '--processed_dataset_interim', processed_dataset_interim],\n",
    "    inputs=[processed_dataset_interim.as_input('processed_data_interim')],\n",
    "    outputs=[processed_dataset],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='.',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "#Transfer processed results from blob storage to Azure SQL DB.\n",
    "#processed_data_interim is passed as a reference and inserted into Azure SQL DB.\n",
    "#Note: data is inserted into the table specified in the SqlDataReference above.\n",
    "transfer_adlsgen2_to_sql = DataTransferStep(\n",
    "    name='save-data-sql',\n",
    "    source_data_reference=processed_dataset_interim,\n",
    "    destination_data_reference=sql_data_ref,\n",
    "    compute_target=data_factory_compute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-basics",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-purse",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_filter_data_step, get_profile_data_step, process_data_step, save_data_adls_step, transfer_adlsgen2_to_sql])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-remark",
   "metadata": {},
   "source": [
    "### Create Experiment and Run Pipeline\n",
    "Define a new experiment (logical container for pipeline runs) and execute the pipeline. You can modify the values of pipeline parameters here when submitting a new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, 'pipeline-development')\n",
    "run = experiment.submit(pipeline, pipeline_parameters={'filter_parameter': 102})\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-appeal",
   "metadata": {},
   "source": [
    "### Publish Pipeline\n",
    "Create a published version of your pipeline that can be triggered via a REST API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name = 'my_pipeline',\n",
    "                                     description = 'Sample pipeline that queries Azure SQL DB, processes data, and exports results to ADLS Gen2/Azure SQL DB',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-president",
   "metadata": {},
   "source": [
    "### Remote Pipeline Trigger\n",
    "Trigger the published pipeline using a call to the REST API. Note: you must use service principal credentials to authenticate this call. The Client ID, Client Secret, Tenant ID, and pipeline endpoint are stored as environment variables prior to execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline execution via REST endpoint requires AAD Token (obtained here from service principal)\n",
    "#Relevant docs:\n",
    "#https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-pipelines\n",
    "#https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "#Service principal creds stored as environment vars\n",
    "client_id = os.environ.get('client_id')\n",
    "tenant_id = os.environ.get('tenant_id')\n",
    "service_principal_password = os.environ.get('service_principal_password')\n",
    "pipeline_endpoint = os.environ.get('pipeline_endpoint')\n",
    "\n",
    "#Leverage ADAL library for obtaining token\n",
    "from adal import AuthenticationContext\n",
    "\n",
    "client_id = client_id\n",
    "client_secret = service_principal_password\n",
    "resource_url = \"https://login.microsoftonline.com\"\n",
    "tenant_id = tenant_id\n",
    "authority = \"{}/{}\".format(resource_url, tenant_id)\n",
    "\n",
    "auth_context = AuthenticationContext(authority)\n",
    "token_response = auth_context.acquire_token_with_client_credentials(\"https://management.azure.com/\", client_id, client_secret)\n",
    "\n",
    "#Format token response for API request to pipeline\n",
    "headers = {'Authorization': 'Bearer {}'.format(token_response['accessToken'])}\n",
    "\n",
    "#Trigger remote pipeline run\n",
    "#Pipeline endpoint can be obtained from AML portal as well\n",
    "response = requests.post(pipeline_endpoint,\n",
    "                         headers=headers,\n",
    "                         json={\"ExperimentName\": \"REST_Pipeline_Trigger_Test\", \"ParameterAssignments\": {\"filter_parameter\": 105}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
